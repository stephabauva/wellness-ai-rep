@context{domain:file_manager, feature_group:advanced_processing, format:shdl, version:2.0.0, complexity:high, enforcement:strict}
@meta{last_updated:"2025-01-01T10:00:00Z", validation_status:complete, token_estimate:850, critical_paths:10, processing_priority:1}

#FILE_COMPRESSION{confidence:1.0, integration_status:complete, @critical}
  ##feature_definition{id:file_compression_root, type:optimization_system, confidence:1.0}
    "Browser-based file compression with TypeScript implementation"
    →primary_user_workflow[select_file→compress→upload_compressed]
    →dependencies[compression_service, browser_apis, compression_algorithms]
    →user_workflows[auto_compression, manual_compression, compression_settings]

  @cluster{core_workflow, confidence:1.0}
    ##compression_workflow{id:primary_compression_flow, type:data_optimization, confidence:1.0}
      "Browser-based file compression process"
      @processing{
        file_selected→check_compressibility→apply_compression→
        measure_ratio→return_compressed_file→upload_compressed
      }
      @data_flow{
        entry:[UniversalFileService.compressFile],
        processing:[FileCompressionService],
        algorithms:[gzip_browser_based],
        output:[compressed_File_object]
      }

    ##compression_decision{id:smart_compression, type:optimization_logic, confidence:1.0}
      "Intelligent compression decision making"
      @decision_factors{
        file_type:"text-based files compress well",
        file_size:"larger files benefit more",
        already_compressed:"skip .zip, .jpg, etc",
        performance:"balance speed vs ratio"
      }
  @/cluster

  @cluster{compression_service, confidence:1.0}
    ##typescript_implementation{id:browser_compression, type:client_service, confidence:1.0}
      FileCompressionService{
        path:"client/src/services/file-compression.ts",
        main_function:"compressFile",
        algorithms:{
          text_files:"gzip compression",
          general:"browser-based compression",
          level:"default compression level"
        },
        capabilities:{
          streaming:"supported for large files",
          progress:"callback-based updates",
          cancellation:"AbortController support"
        },
        status:active
      },
      compression_logic{
        input:"File object",
        process:{
          read:"FileReader API",
          compress:"compression algorithm",
          package:"new File object"
        },
        output:{
          compressedFile:"File",
          compressionRatio:"number",
          originalSize:"number",
          compressedSize:"number"
        }
      }

    ##compression_options{id:configuration, confidence:1.0}
      CompressionOptions{
        interface:{
          compressionLevel:"1-9",
          algorithm:"gzip | deflate",
          chunkSize:"for streaming",
          progressCallback:"optional"
        },
        defaults:{
          level:"default (6)",
          algorithm:"gzip",
          streaming:"auto for large files"
        }
      }
  @/cluster

  @cluster{integration_layer, confidence:1.0}
    ##universal_service_routing{id:compression_router, confidence:1.0}
      UniversalFileService{
        path:"client/src/services/universal-file-service.ts",
        compression_routing:{
          default:"TypeScript compression",
          large_files:"attempt Go acceleration",
          fallback:"original file if fails"
        },
        error_handling:{
          compression_failure:"return original",
          logging:"detailed error info",
          user_impact:"minimal"
        }
      }
  @/cluster

  @cluster{browser_apis, confidence:1.0}
    ##web_apis_used{id:browser_capabilities, confidence:1.0}
      file_apis{
        FileReader:"read file content",
        File_constructor:"create compressed file",
        Blob:"intermediate data handling",
        ArrayBuffer:"binary data processing"
      },
      compression_apis{
        CompressionStream:"native browser API",
        alternatives:"pako.js or similar",
        compatibility:"modern browsers only"
      }
  @/cluster

  @cluster{compression_strategies, confidence:1.0}
    ##file_type_strategies{id:type_specific_compression, confidence:1.0}
      text_based{
        types:["json", "xml", "csv", "txt"],
        expected_ratio:"70-90% reduction",
        algorithm:"gzip optimal"
      },
      already_compressed{
        types:["jpg", "png", "zip", "mp4"],
        strategy:"skip compression",
        reason:"minimal gains"
      },
      binary_files{
        types:["pdf", "doc", "xls"],
        expected_ratio:"20-50% reduction",
        consideration:"time vs benefit"
      }
  @/cluster

  @cluster{performance_optimization, confidence:1.0}
    ##streaming_compression{id:large_file_handling, confidence:1.0}
      chunked_processing{
        trigger:"files > 10MB",
        chunk_size:"1MB default",
        benefits:"prevents memory overflow",
        progress:"real-time updates"
      },
      web_workers{
        status:"not implemented",
        benefit:"non-blocking compression",
        future:"planned enhancement"
      }
  @/cluster

  @cluster{compression_metrics, confidence:1.0}
    ##result_tracking{id:compression_analytics, confidence:1.0}
      metrics_collected{
        compression_ratio:"compressed/original",
        time_taken:"processing duration",
        algorithm_used:"gzip/deflate",
        file_type:"for optimization"
      },
      usage_patterns{
        logging:"console logs only",
        analytics:"not tracked",
        optimization:"manual currently"
      }
  @/cluster

  @cluster{error_handling, confidence:1.0}
    ##failure_scenarios{id:compression_errors, confidence:1.0}
      common_errors{
        memory_overflow:"large file issues",
        invalid_file:"corrupted input",
        browser_limits:"API restrictions"
      },
      recovery_strategy{
        default:"return original file",
        logging:"detailed error info",
        user_notification:"minimal disruption"
      }
  @/cluster

@processing{
  flow_sequence:[
    file_input, type_check, compression_decision,
    algorithm_selection, compression_execution,
    result_packaging, metric_collection
  ],
  critical_paths:[
    compression_execution, error_recovery
  ],
  performance_bottlenecks:[
    large_file_reading, compression_algorithm,
    memory_usage
  ]
}

@dependencies{
  frontend:[
    FileCompressionService, UniversalFileService,
    File_API, CompressionStream_API
  ],
  algorithms:[
    gzip, deflate, browser_native_compression
  ],
  libraries:[
    potentially_pako.js_or_similar
  ]
}

@integration_evidence{
  verified_flows:[
    "Text files compress successfully",
    "Compression integrated with upload",
    "Large files handled without crash",
    "Compression metrics calculated"
  ],
  performance_tested:[
    "JSON files: 80%+ compression",
    "XML files: 70%+ compression",
    "Already compressed: skipped"
  ]
}